
def test():
    ...  
def tokenizer():
    #æ®µè©åˆ†æ éœ€è¦ç¶²è·¯ï¼
    from pandas import DataFrame
    from collections import Counter
    from .models import analysed_news as news

	
    #print(news.db_get(news_id=(7315,8614171)))

    print("ğŸ”¥è¼‰å…¥æ–·è©æ¨¡å‹...çœŸæµªè²»æ™‚é–“")
    from ckip_transformers.nlp import CkipWordSegmenter, CkipPosTagger, CkipNerChunker
    print("ğŸ”¥è¼‰å…¥æ–·è©æ¨¡å‹å®Œæˆï¼")

    # model="albert-tiny" æœ€å°æ¨¡å‹ï¼Œæ–·è©é€Ÿåº¦æ¯”è¼ƒå¿«ï¼ŒçŠ§ç‰²ä¸€äº›ç²¾ç¢ºåº¦
    # model="bert-base" æœ€å¤§æ¨¡å‹ï¼Œæ–·è©æº–ç¢ºï¼Œä¸è¦ç”¨CPUä¾†ç®—
    ws = CkipWordSegmenter(model="bert-base") 
    pos = CkipPosTagger(model="bert-base")
    ner = CkipNerChunker(model="bert-base")
    

    got_news_dict:DataFrame = news.db_get_rowNews_DataFrame()

    # Word Segmentation é€²è¡Œåˆ†è©
    tokens = ws(got_news_dict.content)
    # Word Segmentation é€²è¡Œåˆ†è©
    tokens = ws(got_news_dict.content)
    # POS åˆ†æè©æ€§
    tokens_pos = pos(tokens)

    # word pos pair å°‡1å’Œ2é»åœ¨ä¸€èµ·
    word_pos_pair = [list(zip(w, p)) for w, p in zip(tokens, tokens_pos)]

    # NERå‘½ åå¯¦é«”è¾¨è­˜
    entity_list = ner(got_news_dict.content)

    # éæ¿¾æ¢ä»¶:å…©å€‹å­—ä»¥ä¸Š ç‰¹å®šçš„è©æ€§
    # allowPOS éæ¿¾æ¢ä»¶: ç‰¹å®šçš„è©æ€§
    allowPOS = ['Na', 'Nb', 'Nc', 'VC']

    tokens_v2 = []
    for wp in word_pos_pair:
        tokens_v2.append([w for w, p in wp if (len(w) >= 2) and p in allowPOS])

    # Insert tokens into dataframe (æ–°å¢æ–·è©è³‡æ–™æ¬„ä½)
    got_news_dict['tokens'] = tokens
    got_news_dict['tokens_v2'] = tokens_v2
    got_news_dict['entities'] = entity_list
    got_news_dict['token_pos'] = word_pos_pair

    # Calculate word count (frequency) è¨ˆç®—å­—é »(æ¬¡æ•¸)
    def word_frequency(wp_pair):
        filtered_words = []
        for word, pos in wp_pair:
            if (pos in allowPOS) & (len(word) >= 2):
                filtered_words.append(word)
            #print('%s %s' % (word, pos))
        counter = Counter(filtered_words)
        return counter.most_common(200)

    keyfreqs = []
    for wp in word_pos_pair:
        topwords = word_frequency(wp)
        keyfreqs.append(topwords)
    got_news_dict['top_key_freq'] = keyfreqs

    # Abstract (summary) and sentimental score(æ‘˜è¦èˆ‡æƒ…ç·’åˆ†æ•¸)
    summary = []
    sentiment = []
    for text in got_news_dict.content:
        summary.append("æš«ç„¡")
        sentiment.append("æš«ç„¡")
    got_news_dict['summary'] = summary
    got_news_dict['sentiment'] = sentiment
    
    
    # å°‡ DataFrame å¯«å…¥è³‡æ–™åº«
    if news.db_bulk_update_DataFrame(got_news_dict):
        print("å·²å„²å­˜æ–·è©åˆ†æ")
    else:
        print("â—æ–·è©åˆ†æå„²å­˜å¤±æ•—")
            
# æ­¤éƒ¨åˆ†è² è²¬å°‡ç†±é–€è©çµæœå„²å­˜ç‚ºcsvæª”æ¡ˆ
# df_top_group_words = pd.DataFrame(top_group_words, columns = ['category','top_keys'])
# Part3_file_name = "ç†±é–€è©çµæœ_"+datetime.now().strftime("%m%d_%H%M") +".csv"
# df_top_group_words.to_csv(Part3_file_name, index=False, encoding="utf-8-sig")
# print("å·²å°‡ CSV æª”æ¡ˆå„²å­˜åœ¨ï¼š%s\næª”æ¡ˆåç¨±ï¼š%s" % (where, Part3_file_name))