from fake_useragent import UserAgent
import requests
from bs4 import BeautifulSoup
from .models import analysed_news as news
import time

# åœç”¨æ©Ÿåˆ¶ï¼šä¸€æ¬¡çˆ¬å–æŸå€‹æ™‚é–“æ®µå…§æ‰€æœ‰æ–°è
# from datetime import datetime, timedelta
# gole_Time = datetime.now() - timedelta(days=1)
# page_time = datetime.strptime(news_dict['date'], '%Y-%m-%d %H:%M')

user_agent = UserAgent()
def web_requester(url: str) -> BeautifulSoup|None:
    """ç¶²é çˆ¬èŸ²
    Args:
        url (str): ç¶²é é€£çµ
    Returns:
        BeautifulSoup|None: çˆ¬å–å…§å®¹ï¼Œè‹¥å¤±æ•—å›å‚³None
        
    """
    try:
        req = requests.get(
            url, headers={'user-agent': user_agent.random}, timeout=10)
        req.raise_for_status()  # å¦‚æœè¿”å›ç‹€æ…‹ç¢¼ä¸æ­£å¸¸ï¼Œå‰‡æœƒè§¸ç™¼ç•°å¸¸
        time.sleep(10)  # æ‰€æœ‰ç¶²ç«™å‘¼å«çš„æ“ä½œå¿…é ˆä¼‘æ¯10ç§’å†é€²è¡Œä¸‹ä¸€æ¬¡çˆ¬å–
        return BeautifulSoup(req.text, 'lxml')
    
    except requests.exceptions.RequestException as e:
        print(f"â—core/news_scraper/web_requester çˆ¬å–å¤±æ•—: {e}")
        return None

def news_story_extract(news_link: str) -> dict:
    in_page = web_requester('https://udn.com'+news_link)
    if in_page:
        # é‡è¦ï¼šreplace("&", "&amp;")æ˜¯å¿…è¦çš„ï¼Œç¶²ç«™ä¼ºæœå™¨æœƒé‡å° & å’Œ &amp å‚³è¼¸å…©å¼µä¸ä¸€æ¨£çš„åœ–ç‰‡ï¼Œæ“šè§€å¯Ÿï¼Œå¥½åƒæ˜¯å¤§åœ–å’Œç¸®å°åœ–ï¼Œç¸®å°åœ–æ‡‰è©²æ˜¯ç‚ºäº†ä¸å ç”¨è³‡æºçš„ç‰ˆæœ¬
        try:
            return{
                "date": in_page.find('div', {"class": "article-content__subinfo"}).find('section', {"class": "authors"}).find('time', {"class": "article-content__time"}).get_text('', strip=True),
                "category": in_page.find('nav', {"class": "article-content__breadcrumb"}).find_all("a")[1].get_text('', strip=True),
                "content": in_page.find('section', {"class": "article-content__editor"}).get_text('', strip=True),
            }
        except AttributeError as e:
            print(f"â—core/news_scraper/news_story_extract æ‰¾ä¸åˆ°å°æ‡‰è³‡æ–™ï¼Œå›å‚³ç©ºå­—å…¸: {e}")
            return {}
        except IndexError as e:
            print(f"â—core/news_scraper/news_story_extract å…§æ–‡categoryçš„find_all()[1]å‡ºéŒ¯ï¼Œå›å‚³ç©ºå­—å…¸: {e}")
            return {}

def news_collector_one() -> bool:
    """
    å¾ã€è¯åˆæ–°èç¶²ã€‘çš„ã€å³æ™‚ã€‘é é¢åº•ä¸‹\n
    æŠ“å–é é¢å›å‚³çš„ã€å¨›æ¨‚ã€‘ã€ç§‘æŠ€ã€‘é¡æ–°è(æ•¸é‡å› å„ç¨®æƒ…æ³å—å½±éŸ¿)
    ä¸¦å­˜å…¥è³‡æ–™åº«
    Returns:
        bool: æ˜¯å¦æˆåŠŸ
    """
    print("é–‹å§‹çˆ¬å–ã€å¨›æ¨‚ã€‘ã€ç§‘æŠ€ã€‘é¡æ–°è")
    pages = [web_requester('https://udn.com/news/breaknews/1/8#breaknews'),web_requester('https://udn.com/news/breaknews/1/13#breaknews')]
    for page in pages:
        if(page):
            news_counter = 0
            for each_news in page.find_all('a', {"class": "story-list__image--holder", 'data-content_level': "é–‹æ”¾é–±è®€"}):
                news_dict = {}
                if each_news.get('href'):
                    news_link = each_news.get('href').replace(
                        "?from=udn-ch1_breaknews-1-99-news", "")
                    news_id = (news_link.split("/")[3], news_link.split("/")[4])

                    if news.db_is_news_exists(news_id):
                        print(f"ğŸ”¸å·²æ”¶éŒ„æ–°èï¼š{news_id}")
                    else:
                        news_dict = {
                            "title": each_news.get('aria-label'),
                            "photo_link": each_news.find('source', {"type": "image/webp"}).get('srcset').replace("&", "&amp;")
                        }| news_story_extract(news_link)
                        if news.db_update(news_id, news_dict):
                            print("æ–°æ”¶éŒ„æ–°èï¼š", news_id)
                            news_counter += 1
                        else:
                            print("â—æ”¶éŒ„æ–°èå¤±æ•—ï¼š", news_id)
            print(f"æœ¬æ¬¡ä¸€å…±æ–°æ”¶éŒ„{news_counter}ä»½æ–°è\n")

def news_collector_two() -> bool:
    """
    å¾ã€è¯åˆæ–°èç¶²ã€‘çš„â¬‡ï¸å°ˆæ¬„åº•ä¸‹\n
    ã€è¦èã€‘,ã€é‹å‹•ã€‘,ã€å…¨çƒã€‘,ã€ç¤¾æœƒã€‘,ã€åœ°æ–¹ã€‘,ã€å…©å²¸ã€‘,ã€Oopsã€‘,\n
    ã€ç”¢ç¶“ã€‘,ã€è‚¡å¸‚ã€‘,ã€ç”Ÿæ´»ã€‘,ã€æ–‡æ•™ã€‘,ã€è©•è«–ã€‘,ã€æ—…éŠã€‘\n
    æŠ“å–é é¢å›å‚³çš„æ–°è(æ•¸é‡å› å„ç¨®æƒ…æ³å—å½±éŸ¿)
    ä¸¦å­˜å…¥è³‡æ–™åº«
    Returns:
        bool: æ˜¯å¦æˆåŠŸ
    """
    
    
    
    
    
    
    
    
# åœç”¨ï¼šé€éjså…§éƒ¨è«‹æ±‚æ–°èçš„apiæŠ“å–æ–°èï¼Œç”¨é€”ç‚ºä¸€æ¬¡é‹è¡Œç„¡é™æŠ“å–æ–°èï¼Œç›´åˆ°æ¢ä»¶æ»¿è¶³
# åœç”¨åŸå› ï¼šä¸ç©©å®šï¼Œæ²’å¿…è¦
    # news_counter = 0
    # print("â—æ­¤è™•é–‹å§‹ä½¿ç”¨jså…§éƒ¨è«‹æ±‚æ–°èçš„apiæŠ“å–æ–°èï¼Œå¯èƒ½ä¸ç©©å®šâ—")
    # print(f"åœæ­¢æ¢ä»¶ï¼šæŠ“å–æ–°èæ™‚é–“æ—©æ–¼24å°æ™‚å‰ï¼Œæˆ–æ˜¯æŠ“å–åˆ°ç¬¬10é ")
    # print("gole_Time = ", gole_Time)
    # print("æ¢ä»¶ä¿®æ”¹è™•ï¼šANANews\\core\\news_scraper.py\n")
    # page_counter = 2
    # news_dict = {}
    # while True:
    #     if page_time < gole_Time:
    #         break
    #     if page_counter >= 10:
    #         break

    #     url = f"https://udn.com/api/more?page={page_counter}&id=&channelId=1&cate_id=0&type=breaknews"
    #     page_counter += 1

    #     page = requests.get(
    #         url, headers={'user-agent': user_agent.random}, timeout=5).json()
    #     time.sleep(1)
    #     for item in page["lists"]:

    #         news_link = item["titleLink"].replace(
    #             "?from=udn-ch1_breaknews-1-0-news", "")
    #         news_id = (int(news_link.split("/")
    #                    [3]), int(news_link.split("/")[4]))
    #         if news.db_is_news_exists(news_id):
    #             page_time = datetime.strptime(
    #                 news.db_get(news_id)["date"], '%Y-%m-%d %H:%M')
    #             print(f"ğŸ”¸å·²æ”¶éŒ„æ–°èï¼š{news_id}")
    #         else:
    #             in_url = 'https://udn.com'+news_link
    #             in_req = requests.get(
    #                 in_url, headers={'user-agent': user_agent.random}, timeout=10)
    #             in_page = BeautifulSoup(in_req.text, 'lxml')

    #             news_dict = {
    #                 "date": item['time']["date"],
    #                 "category": in_page.find('nav', {"class": "article-content__breadcrumb"}).find_all("a")[1].get_text('', strip=True),
    #                 "title": item['title'],
    #                 "content": in_page.find('section', {"class": "article-content__editor"}).get_text('', strip=True),
    #                 "photo_link": item['url']}
    #             news.db_update(news_id, news_dict)
    #             print("æ–°æ”¶éŒ„æ–°èï¼š", news_id)
    #             news_counter += 1
    #             page_time = datetime.strptime(
    #                 news_dict['date'], '%Y-%m-%d %H:%M')
    #             time.sleep(1)

    # print(f"æœ¬æ¬¡ä¸€å…±æ–°æ”¶éŒ„{news_counter}ä»½æ–°è")
