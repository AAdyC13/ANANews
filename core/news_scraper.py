
def news_scraper():
    import requests
    from bs4 import BeautifulSoup
    from fake_useragent import UserAgent
    # import pandas as pd
    from .models import analysed_news as news
    from datetime import datetime, timedelta
    import time

    user_agent = UserAgent()
    gole_Time = datetime.now() - timedelta(days=1)
    # åœ¨ã€è¯åˆæ–°èç¶²ã€‘çš„ã€å³æ™‚ã€‘ä¸»é ï¼Œåº•ä¸‹æœ‰ã€ä¸åˆ†é¡ã€‘å³æ™‚åˆ—è¡¨ï¼Œæˆ‘æ‰“ç®—æ‰¹é‡æŠ“é€™è£¡çš„æ–°è
    url = 'https://udn.com/news/breaknews/1/99#breaknews'
    req = requests.get(
        url, headers={'user-agent': user_agent.random}, timeout=5)
    page = BeautifulSoup(req.text, 'lxml')
    news_dict = {}
    news_counter = 0

    for single_news in page.find_all('a', {"class": "story-list__image--holder", 'data-content_level': "é–‹æ”¾é–±è®€"}):
        if single_news.get('href'):
            news_link = single_news.get('href').replace(
                "?from=udn-ch1_breaknews-1-99-news", "")
            news_id = (news_link.split("/")[3], news_link.split("/")[4])

            if news.db_is_news_exists(news_id):
                page_time = datetime.strptime(
                    news.db_get(news_id)["date"], '%Y-%m-%d %H:%M')
                print(f"ğŸ”¸å·²æ”¶éŒ„æ–°èï¼š{news_id}")
            else:

                in_url = 'https://udn.com'+news_link
                in_req = requests.get(
                    in_url, headers={'user-agent': user_agent.random}, timeout=50)
                in_page = BeautifulSoup(in_req.text, 'lxml')
                # é‡è¦ï¼šreplace("&", "&amp;")æ˜¯å¿…è¦çš„ï¼Œç¶²ç«™ä¼ºæœå™¨æœƒé‡å° & å’Œ &amp å‚³è¼¸å…©å¼µä¸ä¸€æ¨£çš„åœ–ç‰‡ï¼Œ
                # æ“šè§€å¯Ÿï¼Œå¥½åƒæ˜¯å¤§åœ–å’Œç¸®å°åœ–ï¼Œç¸®å°åœ–æ‡‰è©²æ˜¯ç‚ºäº†ä¸å ç”¨è³‡æºçš„ç‰ˆæœ¬
                news_picture = single_news.find(
                    'source', {"type": "image/webp"})

                news_dict = {"date": in_page.find('div', {"class": "article-content__subinfo"}).find('section', {"class": "authors"}).find('time', {"class": "article-content__time"}).get_text('', strip=True),
                             "category": in_page.find('nav', {"class": "article-content__breadcrumb"}).find_all("a")[1].get_text('', strip=True),
                             "title": single_news.get('aria-label'),

                             # é‡è¦ï¼šé€™è£¡çš„å…§æ–‡ç„¡èª¤ï¼Œä½†æ²’æœ‰åˆ†æ®µï¼Œæ‰€ä»¥è‹¥ä¹‹å¾Œè¦è®“é€™å€‹æ–‡æœ¬è‡ªå¸¶åˆ†æ®µï¼Œå¯ä»¥å›ä¾†èª¿æ•´
                             "content": in_page.find('section', {"class": "article-content__editor"}).get_text('', strip=True),

                             "photo_link": news_picture.get('srcset').replace("&", "&amp;")}
                news.db_update(news_id, news_dict)
                print("æ–°æ”¶éŒ„æ–°èï¼š", news_id)
                page_time = datetime.strptime(
                    news_dict['date'], '%Y-%m-%d %H:%M')
                news_counter += 1
                time.sleep(1)
    print(f"æœ¬æ¬¡ä¸€å…±æ–°æ”¶éŒ„{news_counter}ä»½æ–°è\n")

    news_counter = 0
    print("â—æ­¤è™•é–‹å§‹ä½¿ç”¨jså…§éƒ¨è«‹æ±‚æ–°èçš„apiæŠ“å–æ–°èï¼Œå¯èƒ½ä¸ç©©å®šâ—")
    print(f"åœæ­¢æ¢ä»¶ï¼šæŠ“å–æ–°èæ™‚é–“æ—©æ–¼24å°æ™‚å‰ï¼Œæˆ–æ˜¯æŠ“å–åˆ°ç¬¬10é ")
    print("gole_Time = ", gole_Time)
    print("æ¢ä»¶ä¿®æ”¹è™•ï¼šANANews\\core\\news_scraper.py\n")
    page_counter = 2
    news_dict = {}
    while True:
        if page_time < gole_Time:
            break
        if page_counter >= 10:
            break

        url = f"https://udn.com/api/more?page={page_counter}&id=&channelId=1&cate_id=0&type=breaknews"
        page_counter += 1

        page = requests.get(
            url, headers={'user-agent': user_agent.random}, timeout=5).json()
        time.sleep(1)
        for item in page["lists"]:

            news_link = item["titleLink"].replace(
                "?from=udn-ch1_breaknews-1-0-news", "")
            news_id = (int(news_link.split("/")
                       [3]), int(news_link.split("/")[4]))
            if news.db_is_news_exists(news_id):
                page_time = datetime.strptime(
                    news.db_get(news_id)["date"], '%Y-%m-%d %H:%M')
                print(f"ğŸ”¸å·²æ”¶éŒ„æ–°èï¼š{news_id}")
            else:
                in_url = 'https://udn.com'+news_link
                in_req = requests.get(
                    in_url, headers={'user-agent': user_agent.random}, timeout=10)
                in_page = BeautifulSoup(in_req.text, 'lxml')

                news_dict = {
                    "date": item['time']["date"],
                    "category": in_page.find('nav', {"class": "article-content__breadcrumb"}).find_all("a")[1].get_text('', strip=True),
                    "title": item['title'],
                    "content": in_page.find('section', {"class": "article-content__editor"}).get_text('', strip=True),
                    "photo_link": item['url']}
                news.db_update(news_id, news_dict)
                print("æ–°æ”¶éŒ„æ–°èï¼š", news_id)
                news_counter += 1
                page_time = datetime.strptime(
                    news_dict['date'], '%Y-%m-%d %H:%M')
                time.sleep(1)

    print(f"æœ¬æ¬¡ä¸€å…±æ–°æ”¶éŒ„{news_counter}ä»½æ–°è")
