from fake_useragent import UserAgent
import requests
from bs4 import BeautifulSoup
from .models import analysed_news as news
from .models import system_config as sysdb
import time
from datetime import datetime, timedelta
from celery import shared_task
import channels.layers
from asgiref.sync import async_to_sync
channel_layer = channels.layers.get_channel_layer()

user_agent = UserAgent()


@shared_task
def news_scraper_starter(want_category: list, each_num: int) -> bool:
    """
    celeryå‡½æ•¸\n
    å¾ã€è¯åˆæ–°èç¶²ã€‘çš„ã€å³æ™‚ã€‘é é¢åº•ä¸‹\n
    æŠ“å–åŠæ™‚åˆ—è¡¨ä¸­ã€è¦èã€‘,ã€ç¤¾æœƒã€‘,ã€åœ°æ–¹ã€‘,ã€å…¨çƒã€‘,ã€å…©å²¸ã€‘,\n
    ã€ç”¢ç¶“ã€‘,ã€è‚¡å¸‚ã€‘,ã€é‹å‹•ã€‘,ã€ç”Ÿæ´»ã€‘,ã€æ–‡æ•™ã€‘\n
    é¡æ–°è(æ•¸é‡å› å„ç¨®æƒ…æ³å—å½±éŸ¿)ä¸¦å­˜å…¥è³‡æ–™åº«
    Args:
        want_category (list): è¦çˆ¬å–çš„é¡åˆ¥
        each_num (int): å„é¡åˆ¥è¦çˆ¬å–çš„æ–°èæ•¸é‡
    Returns:
        bool: æ˜¯å¦æˆåŠŸ
    """
    def news_collector() -> bool:
        """
        ä¸»ç¨‹åºï¼Œå‘¼å«å–®çˆ¬èŸ²é€ä¸€çˆ¬å–ï¼Œå¤ªå¤šrequestæœƒè¢«å°

        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        news_counter = 0
        for i in range(len(want_category)):
            logs_Sender_Printer(f"é–‹å§‹çˆ¬å–ã€{want_category[i]}ã€‘é¡æ–°è")
            if each_num == 0:
                time.sleep(0.1)
            else:
                inside_counter = 0
                page = web_requester(
                    f"https://udn.com/news/breaknews/1/{want_category[i]}#breaknews")
                if (page):
                    for each_news in page.find_all('a', {"class": "story-list__image--holder", 'data-content_level': "é–‹æ”¾é–±è®€"}):
                        if inside_counter >= each_num:
                            break
                        inside_counter += 1
                        news_dict = {}
                        if each_news.get('href'):
                            parts = each_news.get('href').split("/")
                            news_id1, news_id2 = parts[-2], parts[-1].split("?")[0]
                            news_id = (int(news_id1), int(news_id2))
                            if news.db_is_news_exists(news_id):
                                logs_Sender_Printer(f"ğŸ”¸å·²æ”¶éŒ„æ–°èï¼š{news_id}")
                            else:
                                data = news_story_extract(each_news.get('href'))
                                if data:
                                    news_dict = {
                                        "title": each_news.get('aria-label'),
                                        "photo_link": each_news.find('source', {"type": "image/webp"}).get('srcset').replace("&", "&amp;")
                                    } | data

                                    if news.db_update(news_id, news_dict):
                                        logs_Sender_Printer(f"æ–°æ”¶éŒ„æ–°èï¼š{news_id}")
                                        news_counter += 1
                                    else:
                                        logs_Sender_Printer(f"â—æ”¶éŒ„æ–°èå¤±æ•—ï¼š{news_id}")
                                else:
                                    logs_Sender_Printer(f"â—æ”¶éŒ„æ–°èå¤±æ•—ï¼š{news_id}")

        logs_Sender_Printer(f"âœ…æœ¬æ¬¡ä¸€å…±æ–°æ”¶éŒ„{news_counter}ä»½æ–°è")

    def web_requester(url: str) -> BeautifulSoup | None:
        """
        çˆ¬èŸ²å‡½æ•¸
        Args:
            url (str): ç¶²é é€£çµ
        Returns:
            BeautifulSoup|None: çˆ¬å–å…§å®¹ï¼Œè‹¥å¤±æ•—å›å‚³None

        """
        try:
            req = requests.get(
                url, headers={'user-agent': user_agent.random}, timeout=10)
            req.raise_for_status()  # å¦‚æœè¿”å›ç‹€æ…‹ç¢¼ä¸æ­£å¸¸ï¼Œå‰‡æœƒè§¸ç™¼ç•°å¸¸
            time.sleep(10)  # æ‰€æœ‰ç¶²ç«™å‘¼å«çš„æ“ä½œå¿…é ˆä¼‘æ¯10ç§’å†é€²è¡Œä¸‹ä¸€æ¬¡çˆ¬å–
            return BeautifulSoup(req.text, 'lxml')

        except requests.exceptions.RequestException as e:
            logs_Sender_Printer(f"â—core/news_scraper/web_requester çˆ¬å–å¤±æ•—: {e}")
            return None

    def news_story_extract(news_link: str) -> dict | None:
        """
        è² è²¬çˆ¬å–æ–°èå…§æ–‡

        Args:
            news_link (str): å…§æ–‡éˆæ¥

        Returns:
            dict | None: å›å‚³æ•´ç†å¥½çš„å…§æ–‡ï¼Œè‹¥ç„¡å›å‚³None
        """
        in_page = web_requester("https://udn.com"+news_link)
        if in_page:
            # é‡è¦ï¼šreplace("&", "&amp;")æ˜¯å¿…è¦çš„ï¼Œç¶²ç«™ä¼ºæœå™¨æœƒé‡å° & å’Œ &amp å‚³è¼¸å…©å¼µä¸ä¸€æ¨£çš„åœ–ç‰‡ï¼Œæ“šè§€å¯Ÿï¼Œå¥½åƒæ˜¯å¤§åœ–å’Œç¸®å°åœ–ï¼Œç¸®å°åœ–æ‡‰è©²æ˜¯ç‚ºäº†ä¸å ç”¨è³‡æºçš„ç‰ˆæœ¬
            try:
                return {
                    "date": in_page.find('div', {"class": "article-content__subinfo"}).find('section', {"class": "authors"}).find('time', {"class": "article-content__time"}).get_text('', strip=True),
                    "category": in_page.find('nav', {"class": "article-content__breadcrumb"}).find_all("a")[1].get_text('', strip=True),
                    "content": in_page.find('section', {"class": "article-content__editor"}).get_text('', strip=True),
                }
            except AttributeError as e:
                logs_Sender_Printer(
                    f"â—core/news_scraper/news_story_extract æ‰¾ä¸åˆ°å°æ‡‰è³‡æ–™: {e}")
                return None
            except IndexError as e:
                logs_Sender_Printer(
                    f"â—core/news_scraper/news_story_extract å…§æ–‡categoryçš„find_all('a')[1]å‡ºéŒ¯: {e}")
                return None

    def logs_Sender_Printer(message: str) -> bool:
        """
        å‘asgiä¼ºæœå™¨ç™¼é€WebSocketè¨Šæ¯

        Args:
            message (str): è¦ç™¼é€çš„è¨Šæ¯

        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        try:
            print(message)
            log_message = f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] [news_scraper] {message}"
            async_to_sync(channel_layer.group_send)(
                "celery_logs", {"type": "log_message", "message": log_message}
            )
            return True
        except Exception as ex:
            print(f"â—core/news_scraper/logs_sender éŒ¯èª¤: {ex}")
            return False
    logs_Sender_Printer(f"â„¹ï¸news_scraper_starterä»»å‹™å•Ÿå‹•")
    logs_Sender_Printer(f"â„¹ï¸çˆ¬å–é¡åˆ¥ï¼š{want_category}")
    logs_Sender_Printer(f"â„¹ï¸æ¯é¡æ•¸é‡ï¼š{each_num}")
    news_collector()
    return f"news_scraper_starter complete"

# åœç”¨æ©Ÿåˆ¶ï¼šä¸€æ¬¡çˆ¬å–æŸå€‹æ™‚é–“æ®µå…§æ‰€æœ‰æ–°è
# from datetime import datetime, timedelta
# gole_Time = datetime.now() - timedelta(days=1)
# page_time = datetime.strptime(news_dict['date'], '%Y-%m-%d %H:%M')

# åœç”¨ï¼šé€éjså…§éƒ¨è«‹æ±‚æ–°èçš„apiæŠ“å–æ–°èï¼Œç”¨é€”ç‚ºä¸€æ¬¡é‹è¡Œç„¡é™æŠ“å–æ–°èï¼Œç›´åˆ°æ¢ä»¶æ»¿è¶³
# åœç”¨åŸå› ï¼šä¸ç©©å®šï¼Œæ²’å¿…è¦
    # news_counter = 0
    # print("â—æ­¤è™•é–‹å§‹ä½¿ç”¨jså…§éƒ¨è«‹æ±‚æ–°èçš„apiæŠ“å–æ–°èï¼Œå¯èƒ½ä¸ç©©å®šâ—")
    # print(f"åœæ­¢æ¢ä»¶ï¼šæŠ“å–æ–°èæ™‚é–“æ—©æ–¼24å°æ™‚å‰ï¼Œæˆ–æ˜¯æŠ“å–åˆ°ç¬¬10é ")
    # print("gole_Time = ", gole_Time)
    # print("æ¢ä»¶ä¿®æ”¹è™•ï¼šANANews\\core\\news_scraper.py\n")
    # page_counter = 2
    # news_dict = {}
    # while True:
    #     if page_time < gole_Time:
    #         break
    #     if page_counter >= 10:
    #         break

    #     url = f"https://udn.com/api/more?page={page_counter}&id=&channelId=1&cate_id=0&type=breaknews"
    #     page_counter += 1

    #     page = requests.get(
    #         url, headers={'user-agent': user_agent.random}, timeout=5).json()
    #     time.sleep(1)
    #     for item in page["lists"]:

    #         news_link = item["titleLink"].replace(
    #             "?from=udn-ch1_breaknews-1-0-news", "")
    #         news_id = (int(news_link.split("/")
    #                    [3]), int(news_link.split("/")[4]))
    #         if news.db_is_news_exists(news_id):
    #             page_time = datetime.strptime(
    #                 news.db_get(news_id)["date"], '%Y-%m-%d %H:%M')
    #             print(f"ğŸ”¸å·²æ”¶éŒ„æ–°èï¼š{news_id}")
    #         else:
    #             in_url = 'https://udn.com'+news_link
    #             in_req = requests.get(
    #                 in_url, headers={'user-agent': user_agent.random}, timeout=10)
    #             in_page = BeautifulSoup(in_req.text, 'lxml')

    #             news_dict = {
    #                 "date": item['time']["date"],
    #                 "category": in_page.find('nav', {"class": "article-content__breadcrumb"}).find_all("a")[1].get_text('', strip=True),
    #                 "title": item['title'],
    #                 "content": in_page.find('section', {"class": "article-content__editor"}).get_text('', strip=True),
    #                 "photo_link": item['url']}
    #             news.db_update(news_id, news_dict)
    #             print("æ–°æ”¶éŒ„æ–°èï¼š", news_id)
    #             news_counter += 1
    #             page_time = datetime.strptime(
    #                 news_dict['date'], '%Y-%m-%d %H:%M')
    #             time.sleep(1)

    # print(f"æœ¬æ¬¡ä¸€å…±æ–°æ”¶éŒ„{news_counter}ä»½æ–°è")
